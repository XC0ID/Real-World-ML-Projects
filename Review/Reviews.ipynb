{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbef44d2",
   "metadata": {},
   "source": [
    "# Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b10157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from collections import Counter\n",
    "import re\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU OPTIMIZED - FIXED IMPORTS\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW as TorchAdamW  # Explicit full import\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# =========================\n",
    "# LOAD DATA\n",
    "# =========================\n",
    "\n",
    "df = pd.read_csv(\"Appliances_Reviews.csv\")\n",
    "\n",
    "df[\"sentiment\"] = (df[\"overall\"] > 3).astype(int)\n",
    "df[\"text\"] = df[\"reviewText\"].fillna(\"\") + \" \" + df[\"summary\"].fillna(\"\")\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z ]\",\"\",text)\n",
    "    return text\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f5e6a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# LSTM PIPELINE (same as your file)\n",
    "# =========================\n",
    "\n",
    "counter = Counter()\n",
    "for text in df[\"text\"]:\n",
    "    counter.update(text.split()[:100])\n",
    "\n",
    "vocab = dict(counter.most_common(5000))\n",
    "word2idx = {w:i+1 for i,(w,_) in enumerate(vocab.items())}\n",
    "vocab_size = len(vocab) + 1\n",
    "\n",
    "def text_to_sequence(text):\n",
    "    return [word2idx.get(word, 0) for word in text.split()[:100]]\n",
    "\n",
    "\n",
    "sequences = df[\"text\"].apply(text_to_sequence)\n",
    "X = np.array([seq + [0]*(100-len(seq)) for seq in sequences])\n",
    "y = df['sentiment'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.long).to(device)\n",
    "X_test  = torch.tensor(X_test, dtype=torch.long).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "y_test  = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train,y_train), batch_size=128, shuffle=True)\n",
    "test_loader  = DataLoader(TensorDataset(X_test,y_test), batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ae6ac0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM training complete\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# LSTM MODEL\n",
    "# =========================\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self,vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, 128).to(device)\n",
    "        self.lstm = nn.LSTM(128, 128, batch_first=True, dropout=0.3).to(device)\n",
    "        self.fc = nn.Linear(128, 1).to(device)\n",
    "        self.dropout = nn.Dropout(0.3).to(device)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.embedding(x)\n",
    "        x,_ = self.lstm(x)\n",
    "        x = x[:,-1,:]\n",
    "        x = self.fc(x)\n",
    "        return torch.sigmoid(x.squeeze())\n",
    "\n",
    "lstm_model = LSTMModel(vocab_size).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "lstm_optimizer = optim.Adam(lstm_model.parameters(),lr=0.001, weight_decay=0.01)\n",
    "lstm_losses = []\n",
    "\n",
    "for epoch in range(8):\n",
    "    lstm_model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        lstm_optimizer.zero_grad()\n",
    "        preds = lstm_model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(lstm_model.parameters(), 1.0)\n",
    "        lstm_optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    lstm_losses.append(avg_loss)\n",
    "\n",
    "print(\"LSTM training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13b23abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU EVALUATION FUNCTION\n",
    "def evaluate_gpu(model, test_loader, is_bert=False):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "      \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            if is_bert:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                preds = torch.sigmoid(outputs.logits).squeeze().cpu()\n",
    "            else:\n",
    "                xb = batch[0].to(device)\n",
    "                labels = batch[1].to(device)\n",
    "                preds = model(xb).cpu()\n",
    "            \n",
    "            predictions.extend(preds.numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    predictions = (np.array(predictions) > 0.5).astype(int)\n",
    "    true_labels = np.array(true_labels)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy_score(true_labels, predictions),\n",
    "        'precision': precision_score(true_labels, predictions),\n",
    "        'recall': recall_score(true_labels, predictions),\n",
    "        'f1': f1_score(true_labels, predictions),\n",
    "        'predictions': predictions,\n",
    "        'true_labels': true_labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83c686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# EVALUATE BOTH\n",
    "lstm_results = evaluate_gpu(lstm_model, test_loader)\n",
    "bert_results = evaluate_gpu(bert_model, bert_test_loader, is_bert=True)\n",
    "\n",
    "print(f\"\\nâœ… GPU RESULTS:\")\n",
    "print(f\"LSTM:  Acc={lstm_results['accuracy']:.3f} F1={lstm_results['f1']:.3f}\")\n",
    "print(f\"BERT:  Acc={bert_results['accuracy']:.3f} F1={bert_results['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80293d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULTS TABLE\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': ['LSTM (GPU)', 'BERT (GPU)'],\n",
    "    'Accuracy': [lstm_results['accuracy'], bert_results['accuracy']],\n",
    "    'Precision': [lstm_results['precision'], bert_results['precision']],\n",
    "    'Recall': [lstm_results['recall'], bert_results['recall']],\n",
    "    'F1-Score': [lstm_results['f1'], bert_results['f1']]\n",
    "})\n",
    "print(\"\\nðŸ“Š FINAL GPU RESULTS\")\n",
    "print(results_df.round(4))\n",
    "\n",
    "# GPU OPTIMIZED GRAPHS\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Performance Comparison\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.4\n",
    "colors = ['#FF6B6B', '#4ECDC4']\n",
    "axes[0,0].bar(x - width/2, [lstm_results[m.lower()] for m in metrics], width, \n",
    "              label='LSTM', color=colors[0], alpha=0.8)\n",
    "axes[0,0].bar(x + width/2, [bert_results[m.lower()] for m in metrics], width, \n",
    "              label='BERT', color=colors[1], alpha=0.8)\n",
    "axes[0,0].set_title('ðŸš€ GPU Model Performance', fontsize=14, fontweight='bold')\n",
    "axes[0,0].set_xticks(x)\n",
    "axes[0,0].set_xticklabels(metrics, rotation=45)\n",
    "axes[0,0].legend()\n",
    "\n",
    "# Loss Curves\n",
    "axes[0,1].plot(range(1,9), lstm_losses, 'o-', linewidth=3, label='LSTM', color=colors[0])\n",
    "axes[0,1].plot(range(1,5), bert_losses, 's-', linewidth=3, label='BERT', color=colors[1])\n",
    "axes[0,1].set_title('ðŸ“ˆ Training Loss (GPU)', fontsize=14, fontweight='bold')\n",
    "axes[0,1].set_xlabel('Epoch')\n",
    "axes[0,1].legend()\n",
    "\n",
    "# Confusion Matrices\n",
    "sns.heatmap(confusion_matrix(lstm_results['true_labels'], lstm_results['predictions']), \n",
    "            annot=True, fmt='d', cmap='Reds', ax=axes[1,0])\n",
    "axes[1,0].set_title('LSTM Confusion Matrix')\n",
    "\n",
    "sns.heatmap(confusion_matrix(bert_results['true_labels'], bert_results['predictions']), \n",
    "            annot=True, fmt='d', cmap='Blues', ax=axes[1,1])\n",
    "axes[1,1].set_title('BERT Confusion Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('gpu_model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸŽ‰ GPU Training Complete! Check 'gpu_model_comparison.png'\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a1f22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT MODEL PIPELINE\n",
    "# Add this after your LSTM section\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts.iloc[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\n",
    "bert_model = bert_model.to(device)\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = ReviewDataset(dftext[:int(0.8*len(dftext))], dfsentiment[:int(0.8*len(dfsentiment))], tokenizer)\n",
    "test_dataset = ReviewDataset(dftext[int(0.8*len(dftext)):], dfsentiment[int(0.8*len(dfsentiment)):], tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Train function\n",
    "def train_bert(model, train_loader, val_loader, epochs=3):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Evaluate function\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    total_loss = 0\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs.logits.squeeze(), labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.sigmoid(outputs.logits).cpu().numpy()\n",
    "            predictions.extend(preds)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    predictions = (np.array(predictions) > 0.5).astype(int)\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions)\n",
    "    recall = recall_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    \n",
    "    print(f'Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "# Train the model\n",
    "print(\"Training BERT...\")\n",
    "bert_model = train_bert(bert_model, train_loader, test_loader, epochs=3)\n",
    "print(\"BERT training complete!\")\n",
    "\n",
    "# Evaluate\n",
    "bert_results = evaluate(bert_model, test_loader)\n",
    "\n",
    "# Assume you have LSTM results stored, e.g., lstm_results = {'accuracy': 0.85, ...}\n",
    "# Replace with actual if available\n",
    "lstm_results = {'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0}  # Placeholder\n",
    "\n",
    "# Results table\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': ['LSTM', 'BERT'],\n",
    "    'Accuracy': [lstm_results['accuracy'], bert_results['accuracy']],\n",
    "    'Precision': [lstm_results['precision'], bert_results['precision']],\n",
    "    'Recall': [lstm_results['recall'], bert_results['recall']],\n",
    "    'F1-Score': [lstm_results['f1'], bert_results['f1']]\n",
    "})\n",
    "print(results_df)\n",
    "\n",
    "# Graphs: Loss curve placeholder (extend train function to log losses)\n",
    "# For comparison bar chart\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.bar(x - width/2, [lstm_results[m.lower()] for m in metrics], width, label='LSTM')\n",
    "ax.bar(x + width/2, [bert_results[m.lower()] for m in metrics], width, label='BERT')\n",
    "\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('LSTM vs BERT Performance')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png')\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix for BERT (example)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.title('BERT Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.savefig('bert_confusion.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
